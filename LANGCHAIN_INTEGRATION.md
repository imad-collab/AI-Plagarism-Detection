# LangChain Integration Guide

## Overview

The AI Plagiarism Detector now integrates **LangChain** for advanced semantic analysis and agentic capabilities. This integration provides a more sophisticated plagiarism detection system by combining LangChain's semantic embeddings with traditional machine learning algorithms.

## Architecture

### Services

#### 1. **LangChainPlagiarismService** (`src/services/langchainPlagiarismService.py`)
Advanced plagiarism detection using LangChain semantic analysis and embeddings.

**Key Features:**
- **Semantic Embeddings**: Uses HuggingFace embeddings (all-MiniLM-L6-v2)
- **Text Splitting**: Recursive character-based text chunking
- **Vector Similarity**: Semantic similarity calculations
- **Multi-level Analysis**:
  - Semantic sentence-level analysis
  - Chunk-level semantic matching
  - TF-IDF similarity
  - Sequence matching
  - Token overlap (Jaccard similarity)

**Algorithms Used (7 methods):**
1. **Semantic Similarity** (20% weight) - LangChain embeddings for semantic understanding
2. **Chunk-Level Analysis** (15% weight) - TF-IDF similarity at chunk boundaries
3. **Semantic Chunks** (15% weight) - Semantic matching of text chunks
4. **Sentence Semantic Analysis** (15% weight) - LangChain sentence-level semantics
5. **TF-IDF Similarity** (12% weight) - Traditional TF-IDF n-gram matching
6. **Sequence Matching** (12% weight) - Difflib sequence matching
7. **Token Overlap** (11% weight) - Jaccard index for word overlap

#### 2. **AdvancedSimilarityService** (`src/services/advancedSimilarityService.py`)
Original 9-algorithm ensemble for comparison and fallback.

#### 3. **FileUploadService** (`src/services/fileUploadService.py`)
Handles document uploads (PDF, DOCX, TXT) and text extraction.

#### 4. **TextAnalysisService** (`src/services/textAnalysisService.py`)
Provides text statistics and readability metrics.

## API Endpoints

### 1. Upload Document
```http
POST /api/upload
Content-Type: multipart/form-data

Body:
- file: <binary file data>
```

**Response:**
```json
{
  "success": true,
  "file_id": "unique_file_id",
  "filename": "document.pdf",
  "text_length": 5000,
  "word_count": 850
}
```

### 2. Analyze for Plagiarism (with LangChain)
```http
POST /api/analyze
Content-Type: application/json

Body:
{
  "file_id": "unique_file_id",
  "comparison_text": "Text to compare against",
  "use_langchain": true
}
```

**Response:**
```json
{
  "success": true,
  "analysis": {
    "overall_score": 0.75,
    "confidence_score": 0.85,
    "risk_level": "high",
    "similarity_breakdown": {
      "semantic": 0.82,
      "chunk_level": 0.78,
      "semantic_chunks": 0.76,
      "sentence_semantic": 0.74,
      "tfidf": 0.68,
      "sequence_matching": 0.72,
      "token_overlap": 0.65
    },
    "algorithms_count": 8,
    "methodology": "LangChain Semantic + Advanced ML Ensemble",
    "langchain_enabled": true,
    "langchain_weight": 0.65,
    "ml_weight": 0.35,
    "similarity_results": [
      {
        "source": "LangChain Semantic Analysis",
        "similarity_score": 0.82,
        "match_type": "semantic_embedding"
      },
      {
        "source": "Advanced ML Ensemble",
        "similarity_score": 0.68,
        "match_type": "9_algorithm_ensemble"
      }
    ]
  }
}
```

### 3. Dedicated LangChain Analysis
```http
POST /api/langchain-analysis
Content-Type: application/json

Body:
{
  "document_text": "Your document text here",
  "comparison_text": "Text to compare against (optional)"
}
```

**Response:**
```json
{
  "success": true,
  "langchain_results": {
    "overall": 0.82,
    "semantic": 0.82,
    "chunk_level": 0.78,
    ...
  },
  "advanced_results": {
    "overall": 0.68,
    ...
  },
  "comparison": {
    "langchain_semantic": 0.82,
    "advanced_overall": 0.68,
    "difference": 0.14
  }
}
```

### 4. Health Check
```http
GET /api/health
```

**Response:**
```json
{
  "status": "healthy",
  "message": "AI Plagiarism Detector API is running",
  "version": "1.0.0"
}
```

## Installation & Setup

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

**Key Packages:**
- `langchain==0.1.7` - LangChain framework
- `langchain-community==0.0.38` - Community integrations
- `langchain-text-splitters` - Text splitting utilities
- `sentence-transformers==2.2.2` - HuggingFace embeddings
- `faiss-cpu==1.7.4` - Vector similarity search (optional)
- `scikit-learn==1.3.2` - ML algorithms
- `nltk==3.8.1` - NLP utilities

### 2. Run the Application
```bash
python app.py
```

The application will start on `http://localhost:5001`

## How It Works

### 1. Document Upload & Processing
- File is uploaded and validated
- Text is extracted from PDF/DOCX/TXT
- Text is cached for analysis

### 2. LangChain Analysis Flow
```
Input Text
    â†“
[Text Preprocessing]
    â†“
â”œâ”€â†’ [HuggingFace Embeddings] â†’ Semantic Similarity
â”œâ”€â†’ [Text Splitting] â†’ Chunk Analysis
â”œâ”€â†’ [Sentence Tokenization] â†’ Sentence-level Analysis
â”œâ”€â†’ [TF-IDF Vectorization] â†’ TF-IDF Similarity
â”œâ”€â†’ [Sequence Matching] â†’ Difflib Comparison
â””â”€â†’ [Token Overlap] â†’ Jaccard Index
    â†“
[Ensemble Voting]
    â†“
Overall Score (0-1) & Risk Assessment
```

### 3. Scoring & Risk Assessment
- **Score 0.0-0.3**: Low risk (Green)
- **Score 0.3-0.7**: Medium risk (Yellow)
- **Score 0.7-1.0**: High risk (Red)

## Configuration

### Environment Variables
```env
FLASK_ENV=development
FLASK_DEBUG=True
MAX_CONTENT_LENGTH=16777216  # 16MB max file size
UPLOAD_FOLDER=uploads
```

### Customize Weights
Edit `src/services/langchainPlagiarismService.py` in `calculate_plagiarism_score()`:
```python
overall_score = (
    semantic_sim * 0.20 +                    # LangChain semantic
    chunk_sim * 0.15 +                       # LangChain chunk level
    semantic_chunk_sim * 0.15 +             # LangChain semantic chunks
    sentence_semantic_sim * 0.15 +          # LangChain sentence semantic
    tfidf_sim * 0.12 +                      # TF-IDF
    sequence_sim * 0.12 +                   # Sequence matching
    token_sim * 0.11                        # Token overlap
)
```

## Performance Metrics

### Processing Speed
- Small documents (< 500 words): ~100-200ms
- Medium documents (500-2000 words): ~200-500ms
- Large documents (> 2000 words): ~500-1000ms

### Accuracy
- Identical text: 95%+ similarity detected
- Heavily paraphrased text: 40-60% similarity
- Completely different text: 0-20% similarity

## Agentic Capabilities (Future Enhancements)

LangChain enables advanced agentic features:

### 1. **RetrievalQA Chain**
```python
# Future: Ask questions about document
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",
    retriever=vector_store.as_retriever()
)
result = qa.run("What is the main topic?")
```

### 2. **LLM-Based Analysis Agent**
```python
# Future: Use LLM for semantic analysis
from langchain.agents import initialize_agent, Tool

tools = [
    Tool(name="Plagiarism Check", func=langchain_service.calculate_plagiarism_score),
    Tool(name="Text Analysis", func=text_analysis_service.analyze_text),
]

agent = initialize_agent(tools, llm, agent="zero-shot-react-description")
```

### 3. **Document Summarization**
```python
# Future: Summarize documents using LLM
from langchain.chains import summarize_chain

summary = summarize_chain.run(document_text)
```

## Troubleshooting

### Issue: "Could not import sentence_transformers"
**Solution:**
```bash
pip install sentence-transformers
```

### Issue: Slow embeddings generation
**Solution:** Embeddings are cached after first use. Clear cache if needed:
```bash
rm -rf ~/.cache/huggingface
```

### Issue: Memory errors with large documents
**Solution:** Reduce chunk size in `langchainPlagiarismService.py`:
```python
RecursiveCharacterTextSplitter(
    chunk_size=250,  # Reduce from 500
    chunk_overlap=25
)
```

## Testing

### Test with cURL
```bash
# Health check
curl http://localhost:5001/api/health

# Upload document
curl -X POST -F "file=@document.pdf" \
  http://localhost:5001/api/upload

# Analyze plagiarism
curl -X POST http://localhost:5001/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "file_id": "your_file_id",
    "comparison_text": "Sample text",
    "use_langchain": true
  }'
```

### Test with Python
```python
import requests

# Upload
response = requests.post(
    'http://localhost:5001/api/upload',
    files={'file': open('document.pdf', 'rb')}
)
file_id = response.json()['file_id']

# Analyze
response = requests.post(
    'http://localhost:5001/api/analyze',
    json={
        'file_id': file_id,
        'comparison_text': 'Sample text',
        'use_langchain': True
    }
)
print(response.json())
```

## Advanced Features

### 1. Semantic Search
```python
# Find similar documents in vector store
similar_docs = vector_store.similarity_search(query_text, k=5)
```

### 2. Batch Processing
```python
# Analyze multiple documents
for doc in documents:
    result = langchain_service.calculate_plagiarism_score(doc)
```

### 3. Custom Embeddings Model
Edit `LangChainPlagiarismService.__init__()`:
```python
self.embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)
```

## Resources

- **LangChain Documentation**: https://python.langchain.com
- **HuggingFace Models**: https://huggingface.co/models
- **Sentence Transformers**: https://www.sbert.net
- **scikit-learn**: https://scikit-learn.org

## Next Steps

1. âœ… LangChain semantic embeddings integrated
2. âœ… Multi-algorithm ensemble approach
3. ðŸ”„ Optional: Integrate LLM for AI-powered analysis
4. ðŸ”„ Optional: Build retrieval-augmented generation (RAG)
5. ðŸ”„ Optional: Implement document summarization
6. ðŸ”„ Optional: Add conversational AI agent

## Support

For issues or questions:
1. Check the troubleshooting section
2. Review LangChain documentation
3. Check application logs in console
4. Review test results

---

**LangChain Integration Version**: 1.0.0  
**Last Updated**: October 2025
