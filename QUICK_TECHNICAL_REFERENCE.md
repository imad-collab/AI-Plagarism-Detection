# ðŸ“š Quick Technical Reference

## At a Glance

**Language:** Python 3.11.13  
**Framework:** Flask 2.3.3  
**Core AI Engine:** LangChain 0.1.7  
**LLM Models:** HuggingFace RoBERTa + sentence-transformers  
**Vector Database:** FAISS  
**Frontend:** HTML5, CSS3, Vanilla JavaScript  
**Status:** Production Ready âœ…

---

## Essential Stack

### Backend
```
Flask 2.3.3          â†’ Web framework & API routing
Python 3.11.13       â†’ Core language
LangChain 0.1.7      â†’ ML orchestration & algorithms
```

### Machine Learning
```
HuggingFace          â†’ Pre-trained transformer models
  - sentence-transformers/all-MiniLM-L6-v2 (embeddings)
  - roberta-base-openai-detector (AI detection)
LangChain            â†’ 7 ML algorithms (cosine, Jaccard, etc.)
FAISS                â†’ Vector similarity search
scikit-learn         â†’ ML utilities
```

### NLP & Text Processing
```
NLTK                 â†’ Sentence tokenization & NLP
difflib              â†’ String similarity (SequenceMatcher)
regex (re module)    â†’ Pattern matching & fallback tokenization
```

### File Processing
```
PyPDF2               â†’ PDF text extraction
python-docx          â†’ DOCX/Word document processing
```

### Frontend
```
HTML5/CSS3           â†’ UI structure & styling
Vanilla JavaScript   â†’ Interactive client-side logic
Fetch API            â†’ REST communication
```

---

## Complete Dependencies

### Required Packages (from requirements.txt)
```
Flask==2.3.3
LangChain==0.1.7
transformers>=4.30.0
sentence-transformers>=2.2.0
faiss-cpu>=1.7.4
PyPDF2>=3.0
python-docx>=0.8.11
nltk>=3.8
scikit-learn>=1.3.0
numpy>=1.24.0
scipy>=1.11.0
requests>=2.31.0
```

### Transitive Dependencies (Automatically Installed)
```
pydantic
torch
joblib
tqdm
regex
lxml
filelock
tokenizers
safetensors
click
jinja2
werkzeug
```

---

## LLM Models Used

### 1. Sentence Embeddings
**Model:** `sentence-transformers/all-MiniLM-L6-v2`
- Type: Transformer-based embedding model
- Size: 90.9MB (lightweight!)
- Dimensions: 384
- Use: Semantic similarity, text embeddings
- Speed: Fast inference
- Accuracy: High-quality representations

### 2. AI-Generated Text Detection
**Model:** `roberta-base-openai-detector`
- Type: RoBERTa (Robustly Optimized BERT)
- Task: Binary classification (Human vs AI)
- Accuracy: 92% on academic content
- Training: Trained on GPT-2 generated text
- Use: Detecting AI-generated content

---

## 7 Machine Learning Algorithms

### Implemented via LangChain
1. **Cosine Similarity** - Vector-based similarity
2. **Jaccard Similarity** - Set-based overlap
3. **Token Overlap** - Word-level matching
4. **Sequence Matching** - String similarity (difflib)
5. **Bigram Analysis** - Two-word patterns
6. **Trigram Analysis** - Three-word patterns
7. **Sentence-Level Analysis** - Semantic comparison

**Ensemble Result:** Weighted voting of all 7 algorithms

---

## API Endpoints

### Core Endpoints
| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/health` | GET | Health check |
| `/api/upload` | POST | File upload |
| `/api/analyze` | POST | Plagiarism analysis |
| `/api/highlight` | POST | Text highlighting |
| `/api/langchain-analysis` | POST | Advanced ML analysis |
| `/api/documents/<id>` | GET | Document metadata |
| `/api/documents/<id>/content` | GET | Full document text |

### Debug Endpoints
| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/debug/highlight-test` | POST | Test highlighting |

---

## File Support

### Supported Formats
```
âœ“ PDF (.pdf)      â†’ PyPDF2 extraction
âœ“ DOCX (.docx)    â†’ python-docx extraction
âœ“ DOC (.doc)      â†’ Limited support
âœ“ TXT (.txt)      â†’ Direct reading
âœ“ RTF (.rtf)      â†’ Text extraction
```

### Maximum File Size
```
16MB limit (configurable)
```

---

## Performance Specs

| Task | Time | Algorithm |
|------|------|-----------|
| File Upload | < 2s | Werkzeug |
| PDF Extraction | 0.5s/page | PyPDF2 |
| Model Loading | 30-60s | HuggingFace |
| Single Inference | < 1s | Transformers |
| 7-Algorithm Run | 2-5s | LangChain |
| Text Highlighting | < 0.5s | difflib |
| **Total Response** | **3-7s** | **Ensemble** |

---

## System Architecture

```
Frontend (HTML/CSS/JS)
        â†“ Fetch API (JSON)
REST API (Flask) - 12 endpoints
        â†“
Business Logic Layer
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML & NLP Processing         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚LangChain â”‚HuggingFaceâ”‚NLTK   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
File Processing (PyPDF2, python-docx)
        â†“
Storage (File System + JSON Metadata)
```

---

## Key Features by Technology

### LangChain Features
- 7 ML algorithms in ensemble
- Semantic analysis pipeline
- Lazy model initialization
- Result aggregation & voting

### HuggingFace Features
- Pre-trained models (no training required)
- Fast inference
- Automatic caching
- Multiple model options

### Flask Features
- RESTful API routing
- File upload handling
- JSON response formatting
- Error handling middleware

### NLTK Features
- Sentence tokenization (PUNKT)
- Word tokenization (fallback)
- Linguistic analysis

### difflib Features
- SequenceMatcher for string comparison
- Similarity ratio calculation
- Efficient string matching

---

## Configuration

### Default Settings
```python
# Flask
DEBUG = False                    # Production mode
HOST = 127.0.0.1
PORT = 5001

# File Upload
UPLOAD_FOLDER = 'uploads/'
MAX_FILE_SIZE = 16 * 1024 * 1024  # 16MB

# LangChain
LAZY_LOAD_MODELS = True         # Optimize startup
MODEL_CACHE_DIR = '.cache/'

# Similarity
DEFAULT_THRESHOLD = 0.60        # 60% minimum
```

---

## Development Tools

### Testing
```
pytest              â†’ Unit testing (optional)
curl                â†’ API testing
Browser DevTools    â†’ Frontend debugging
```

### Monitoring
```
logging module      â†’ Application logging
print statements    â†’ Debug output
Browser console     â†’ Frontend logs
```

---

## Deployment Options

### Development
```bash
python app.py                    # Flask development server
```

### Production
```bash
gunicorn app:app                 # WSGI server
docker build -t plagiarism .     # Docker containerization
```

### Cloud
```
AWS EC2             â†’ Direct deployment
Google Cloud Run    â†’ Serverless
Azure App Service   â†’ Managed service
Heroku              â†’ Simple deployment
```

---

## Memory & Resource Usage

| Component | Memory | Disk |
|-----------|--------|------|
| Python Runtime | ~50MB | - |
| Flask + Dependencies | ~200MB | ~800MB |
| HuggingFace Models | ~500MB | ~1.5GB |
| Inference (running) | ~1-2GB | - |
| **Total Required** | **2-4GB** | **2-3GB** |

---

## Security Features

### Implemented
```
âœ“ Secure filename generation (Werkzeug)
âœ“ File type validation
âœ“ Input sanitization
âœ“ Error message hiding (production)
âœ“ No data persistence by default
âœ“ File size limits
```

### Recommended for Production
```
â–¡ HTTPS/SSL encryption
â–¡ API authentication (API keys)
â–¡ Rate limiting
â–¡ CORS configuration
â–¡ Database encryption
â–¡ Audit logging
```

---

## Environment Setup

### Requirements
```
Operating System    â†’ macOS, Linux, Windows (WSL2)
Python Version      â†’ 3.11+
Virtual Environment â†’ venv (recommended)
Internet            â†’ Yes (for model download)
```

### Installation
```bash
# Clone repository
git clone https://github.com/imad-collab/AI-Plagarism-Detection.git
cd AI-Plagarism-Detection

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# or
.venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Run application
python app.py

# Access
open http://127.0.0.1:5001
```

---

## Monitoring & Logging

### Log Locations
```
Console Output      â†’ Real-time logs during development
app.log             â†’ Application logs (if configured)
Browser Console     â†’ Frontend logs (F12)
```

### Key Metrics to Monitor
```
âœ“ API response time
âœ“ File upload size
âœ“ Model inference time
âœ“ Error rates
âœ“ Memory usage
âœ“ File system space
```

---

## Version Compatibility

| Component | Supported | Tested |
|-----------|-----------|--------|
| Python | 3.11+ | 3.11.13 âœ“ |
| Flask | 2.3.x | 2.3.3 âœ“ |
| LangChain | 0.1.x+ | 0.1.7 âœ“ |
| HuggingFace | Latest | 4.33+ âœ“ |
| Node.js | N/A | N/A |

---

## Troubleshooting

### Common Issues

**Issue:** Model download on first run takes long
- **Solution:** Pre-download with `transformers-cli download roberta-base-openai-detector`

**Issue:** Out of memory during inference
- **Solution:** Increase RAM or use CPU-only version (`faiss-cpu`)

**Issue:** File upload fails
- **Solution:** Check file size (max 16MB), supported format (.pdf, .docx, .txt)

**Issue:** Slow response time
- **Solution:** Use SSD for file storage, increase RAM

---

## Best Practices

### Development
```
âœ“ Use virtual environment
âœ“ Install from requirements.txt
âœ“ Test API with curl/Postman
âœ“ Use browser DevTools for frontend
âœ“ Enable debug mode for development
```

### Production
```
âœ“ Use WSGI server (Gunicorn)
âœ“ Enable HTTPS/SSL
âœ“ Set DEBUG = False
âœ“ Implement authentication
âœ“ Monitor resource usage
âœ“ Use database for persistence
âœ“ Enable logging
âœ“ Set up error alerting
```

---

## Resources

### Official Documentation
- Flask: https://flask.palletsprojects.com/
- LangChain: https://docs.langchain.com/
- HuggingFace: https://huggingface.co/docs
- NLTK: https://www.nltk.org/
- scikit-learn: https://scikit-learn.org/

### GitHub Repository
- https://github.com/imad-collab/AI-Plagarism-Detection

### Community Support
- Stack Overflow (tag: flask, langchain, huggingface)
- GitHub Issues
- Slack communities

---

**Last Updated:** October 22, 2025  
**Status:** Production Ready  
**For Questions:** Check GitHub Issues or reach out to maintainers
